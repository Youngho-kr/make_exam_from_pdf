import pdfplumber
from soynlp.tokenizer import LTokenizer
from collections import Counter
import random
import re
import unicodedata

def split_tokens(tokens):
    pattern = re.compile(r"(\W|\d+)")
    new_tokens = []
    for token in tokens:
        # í† í° ë‚´ì˜ íŠ¹ìˆ˜ ë¬¸ìì™€ ìˆ«ìë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ë¶„ë¦¬
        parts = pattern.split(token)
        # ê³µë°±ì´ ì•„ë‹Œ í† í°ë§Œ ì¶”ê°€
        parts = [part for part in parts if part.strip()]  # ê³µë°± ì œê±° ë° ë¹ˆ ë¬¸ìì—´ í™•ì¸
        if parts:  # ë¶€ë¶„ì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
            new_tokens.extend(parts)
    return new_tokens

def remove_suffix(word):
    # í”íˆ ì‚¬ìš©ë˜ëŠ” ì¡°ì‚¬ë‚˜ ì ‘ë¯¸ì‚¬ ë¦¬ìŠ¤íŠ¸
    suffixes = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ê¹Œì§€', 'ë§Œ', 'ì¡°ì°¨', 'ë„', 'ë°–ì—', 'ë§ˆì €', 'í•œí…Œ', 'ë‘', 'ì´ë‚˜', 'ë‚˜', 'ë¶€í„°', 'ì²˜ëŸ¼', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë§ˆëƒ¥', 'ì»¤ë…•', 'ë³´ë‹¤']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

def generate_questions_korean_soynlp(text, num_questions=3, context_words=100):
    # soynlp í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = LTokenizer()
    
    # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
    tokens = tokenizer.tokenize(text)

    # í† í° ì¶”ê°€ ë¶„ë¦¬
    tokens = split_tokens(tokens)

    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ, í•„ìš”ì— ë”°ë¼ ìˆ˜ì • ê°€ëŠ¥)
    stop_words = [
        'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ê²Œ', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë¡œì„œ', 'ë¡œì¨', 'ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë³´ë‹¤', 'í•˜ê³ ', 'ë“±', 'ë“±ë“±',
        'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ',
        'ê·¸', 'ê·¸ë…€', 'ê·¸ê²ƒ', 'ì €', 'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì—¬ëŸ¬ë¶„', 'ê·¸ë“¤', 'ì œ', 'ë‚´',
        'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ìˆëŠ”',
        'ê²ƒ', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ì—¬ê¸°', 'ì €ê¸°', 'ì–´ë””', 'ì•„ë¬´', 'ë¬´ì—‡', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì–´ë–¤', 'ë‹¤ë¥¸', 'ë˜', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ë§ì´', 'ì¢€', 'ì¡°ê¸ˆ', 'ë‹¤ì‹œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì™œëƒí•˜ë©´',
        'í†µí•´', 'ê°€ì¥', 'ì˜',
        "-", "--", ":", "->", "=", "-", "l", "+", "+", "ğ‘ ", ",", "â€¢", "âˆ’", " ", "!", ".", "(", ")", "âˆ¶", ">", "ğ’Š", "[", "âˆ™", "*", "%", "ğ‘›", "âˆ‘", "â€˜",
        "AI", "ìœµí•©", "ìº¡ìŠ¤í†¤", "ë””ìì¸", "Sogang" ,"ìˆ˜",
        "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z",
        "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
        "the",
        "1", "2", "3", "4", "5", "6", "7", "8", "9", "0",
    ]

    # special_ranges = [
    #     (0x0021, 0x002F),  # Punctuation
    #     (0x003A, 0x0040),  # More punctuation
    #     (0x005B, 0x0060),  # Even more punctuation
    #     (0x007B, 0x007E),  # Guess what? Yes, punctuation
    #     (0x2000, 0x206F),  # General Punctuation
    #     (0x20A0, 0x20CF),  # Currency Symbols
    #     (0x2190, 0x21FF),  # Arrows
    #     (0x2300, 0x23FF),  # Miscellaneous Technical
    #     (0x25A0, 0x25FF),  # Geometric Shapes
    #     (0x2600, 0x26FF),  # Miscellaneous Symbols
    #     (0x2700, 0x27BF),  # Dingbats
    #     (0x2B50, 0x2B59),  # More Miscellaneous Symbols
    #     (0x2900, 0x297F),  # Supplemental Arrows-B
    #     (0x1F300, 0x1F5FF),  # Miscellaneous Symbols and Pictographs
    #     (0x1F600, 0x1F64F),  # Emoticons
    #     (0x1F680, 0x1F6FF),  # Transport and Map Symbols
    # ]
    
    # special_chars = []
    # for start, end in special_ranges:
    #     for code in range(start, end + 1):
    #         if 'CHARACTER' in unicodedata.name(chr(code), ''):
    #             special_chars.append(chr(code))

    #     # special_chars.extend([chr(code) for code in range(start, end + 1) if 'CHARACTER' in unicodedata.name(chr(code), '')])

    # stop_words += special_chars

    # print(stop_words)

    filtered_tokens = []
    for word in tokens:
        if word not in stop_words:
            filtered_tokens.append(word)
        
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    word_freq = Counter(filtered_tokens)
    most_common_words = word_freq.most_common(num_questions)
    
    questions = []
    for word, _ in most_common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        processed_word = remove_suffix(word)
        word_positions = [i for i, w in enumerate(tokens) if remove_suffix(w) == processed_word]

        # ì£¼ì–´ì§„ ìœ„ì¹˜ë“¤ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ í•˜ë‚˜ë¥¼ ì„ íƒ
        if word_positions:
            pos = random.choice(word_positions)
            start = max(0, pos - context_words)
            end = min(len(tokens), pos + context_words + 1)
            context = tokens[start:end]
            blanked_context = ' '.join(context).replace(processed_word, "______")
            questions.append((f"Fill in the blank: {blanked_context}", processed_word))
    
    return questions

def extract_text_from_pdf(pdf_path):
    # PDF íŒŒì¼ì„ ì—´ê³  ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤.
    pdf_pages=[]
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                pdf_pages.append(text)

    return pdf_pages

def main():
    pages = extract_text_from_pdf("ch5.pdf")

    # ë¬¸ì œ ìƒì„±
    questions = []
    for page in pages:
        questions += generate_questions_korean_soynlp(page)

    # ë¬¸ì œ ì¶œë ¥
    for i, (question, answer) in enumerate(questions, 1):
        print(f"Question {i}\n {question}\nAnswer: {answer}\n")



if __name__ == "__main__":
    main()