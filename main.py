import pdfplumber
from soynlp.tokenizer import LTokenizer
from collections import Counter
import random
import re
import unicodedata
import sys

def split_tokens(tokens):
    pattern = re.compile(r"(\W|\d+)")
    new_tokens = []
    for token in tokens:
        # ÌÜ†ÌÅ∞ ÎÇ¥Ïùò ÌäπÏàò Î¨∏ÏûêÏôÄ Ïà´ÏûêÎ•º Î≥ÑÎèÑÏùò ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÎ¶¨
        parts = pattern.split(token)
        # Í≥µÎ∞±Ïù¥ ÏïÑÎãå ÌÜ†ÌÅ∞Îßå Ï∂îÍ∞Ä
        parts = [part for part in parts if part.strip()]  # Í≥µÎ∞± Ï†úÍ±∞ Î∞è Îπà Î¨∏ÏûêÏó¥ ÌôïÏù∏
        if parts:  # Î∂ÄÎ∂ÑÏù¥ ÎπÑÏñ¥ ÏûàÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ÏóêÎßå Ï∂îÍ∞Ä
            new_tokens.extend(parts)
    return new_tokens

def remove_suffix(word):
    # ÌùîÌûà ÏÇ¨Ïö©ÎêòÎäî Ï°∞ÏÇ¨ÎÇò Ï†ëÎØ∏ÏÇ¨ Î¶¨Ïä§Ìä∏
    suffixes = ['ÏùÑ', 'Î•º', 'Ïù¥', 'Í∞Ä', 'ÏùÄ', 'Îäî', 'Ïùò', 'Ïóê', 'ÏúºÎ°ú', 'ÏôÄ', 'Í≥º', 'ÎèÑ', 'Î°ú', 'ÏóêÏÑú', 'ÏóêÍ≤å', 'ÍπåÏßÄ', 'Îßå', 'Ï°∞Ï∞®', 'ÎèÑ', 'Î∞ñÏóê', 'ÎßàÏ†Ä', 'ÌïúÌÖå', 'Îûë', 'Ïù¥ÎÇò', 'ÎÇò', 'Î∂ÄÌÑ∞', 'Ï≤òÎüº', 'Ï≤òÎüº', 'Í∞ôÏù¥', 'ÎßàÎÉ•', 'Ïª§ÎÖï', 'Î≥¥Îã§']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

def generate_questions_korean_soynlp(text, num_questions=1, context_words=100):
    # soynlp ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÉùÏÑ±
    tokenizer = LTokenizer()
    
    # ÌÖçÏä§Ìä∏ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï
    tokens = tokenizer.tokenize(text)

    # ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä Î∂ÑÎ¶¨
    tokens = split_tokens(tokens)

    # Î∂àÏö©Ïñ¥
    stop_words = [
        'ÏùÄ', 'Îäî', 'Ïù¥', 'Í∞Ä', 'ÏùÑ', 'Î•º', 'Ïóê', 'ÏóêÍ≤å', 'ÏóêÏÑú', 'ÏôÄ', 'Í≥º', 'ÎèÑ', 'Îßå', 'Ï≤òÎüº', 'Í∞ôÏù¥', 'Î°úÏÑú', 'Î°úÏç®', 'Î°ú', 'Î∂ÄÌÑ∞', 'ÍπåÏßÄ', 'Î≥¥Îã§', 'ÌïòÍ≥†', 'Îì±', 'Îì±Îì±',
        'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü¨ÎÇò', 'Í∑∏Îü∞Îç∞', 'ÌïòÏßÄÎßå', 'Îî∞ÎùºÏÑú', 'Í∑∏ÎûòÏÑú',
        'Í∑∏', 'Í∑∏ÎÖÄ', 'Í∑∏Í≤É', 'Ï†Ä', 'ÎÇò', 'ÎÑà', 'Ïö∞Î¶¨', 'Ïó¨Îü¨Î∂Ñ', 'Í∑∏Îì§', 'Ï†ú', 'ÎÇ¥',
        'ÌïòÎã§', 'ÎêòÎã§', 'ÏûàÎã§', 'ÏûàÎäî',
        'Í≤É', 'Ïù¥Îü∞', 'Ï†ÄÎü∞', 'Í∑∏Îü∞', 'Ïó¨Í∏∞', 'Ï†ÄÍ∏∞', 'Ïñ¥Îîî', 'ÏïÑÎ¨¥', 'Î¨¥Ïóá', 'Ïù¥Í≤É', 'Ï†ÄÍ≤É', 'Ïñ¥Îñ§', 'Îã§Î•∏', 'Îòê', 'Ïù¥Î†áÍ≤å', 'Í∑∏Î†áÍ≤å', 'Ï†ÄÎ†áÍ≤å', 'ÎßéÏù¥', 'Ï¢Ä', 'Ï°∞Í∏à', 'Îã§Ïãú', 'Í∑∏Îü¨ÎØÄÎ°ú', 'ÏôúÎÉêÌïòÎ©¥',
        'ÌÜµÌï¥', 'Í∞ÄÏû•', 'Ïûò',
        "-", "--", ":", "->", "=", "-", "l", "+", "+", "ùë†", ",", "‚Ä¢", "‚àí", " ", "!", ".", "(", ")", "‚à∂", ">", "ùíä", "[", "‚àô", "*", "%", "ùëõ", "‚àë", "‚Äò",
        "AI", "ÏúµÌï©", "Ï∫°Ïä§ÌÜ§", "ÎîîÏûêÏù∏", "Sogang" ,"Ïàò",
        "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z",
        "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
        "the",
        "1", "2", "3", "4", "5", "6", "7", "8", "9", "0",
    ]

    # special_ranges = [
    #     (0x0021, 0x002F),  # Punctuation
    #     (0x003A, 0x0040),  # More punctuation
    #     (0x005B, 0x0060),  # Even more punctuation
    #     (0x007B, 0x007E),  # Guess what? Yes, punctuation
    #     (0x2000, 0x206F),  # General Punctuation
    #     (0x20A0, 0x20CF),  # Currency Symbols
    #     (0x2190, 0x21FF),  # Arrows
    #     (0x2300, 0x23FF),  # Miscellaneous Technical
    #     (0x25A0, 0x25FF),  # Geometric Shapes
    #     (0x2600, 0x26FF),  # Miscellaneous Symbols
    #     (0x2700, 0x27BF),  # Dingbats
    #     (0x2B50, 0x2B59),  # More Miscellaneous Symbols
    #     (0x2900, 0x297F),  # Supplemental Arrows-B
    #     (0x1F300, 0x1F5FF),  # Miscellaneous Symbols and Pictographs
    #     (0x1F600, 0x1F64F),  # Emoticons
    #     (0x1F680, 0x1F6FF),  # Transport and Map Symbols
    # ]
    
    # special_chars = []
    # for start, end in special_ranges:
    #     for code in range(start, end + 1):
    #         if 'CHARACTER' in unicodedata.name(chr(code), ''):
    #             special_chars.append(chr(code))

    #     # special_chars.extend([chr(code) for code in range(start, end + 1) if 'CHARACTER' in unicodedata.name(chr(code), '')])

    # stop_words += special_chars

    # print(stop_words)

    filtered_tokens = []
    for word in tokens:
        if word not in stop_words:
            filtered_tokens.append(word)
        
    
    # ÏµúÎπà Îã®Ïñ¥
    word_freq = Counter(filtered_tokens)
    most_common_words = word_freq.most_common(num_questions)
    
    questions = []
    for word, _ in most_common_words:
        # Ï°∞ÏÇ¨ Îì± Ï†úÍ±∞
        processed_word = remove_suffix(word)
        word_positions = [i for i, w in enumerate(tokens) if remove_suffix(w) == processed_word]

        # ÎûúÎç§ÏúºÎ°ú ÏÉùÏÑ±
        if word_positions:
            pos = random.choice(word_positions)
            start = max(0, pos - context_words)
            end = min(len(tokens), pos + context_words + 1)
            context = tokens[start:end]
            blanked_context = ' '.join(context).replace(processed_word, "______")
            questions.append((f"Fill in the blank: {blanked_context}", processed_word))
    
    return questions

def extract_text_from_pdf(pdf_path):
    # Í∞Å ÌéòÏù¥ÏßÄ Î≥ÑÎ°ú Ï†ÄÏû•
    pdf_pages=[]
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                pdf_pages.append(text)

    return pdf_pages

def main():
    if len(sys.argv) != 3:
        print("Usage: python main.py [QUESTIONS FOR PAGE] [FILE_NAME]")
        sys.exit(1)

    num_questions = int(sys.argv[1])
    file_path = sys.argv[2]

    pages = extract_text_from_pdf(file_path)

    # Î¨∏Ï†ú ÏÉùÏÑ±
    questions = []
    for page in pages:
        questions += generate_questions_korean_soynlp(page, num_questions=num_questions)

    # Î¨∏Ï†ú Ï∂úÎ†•
    for i, (question, answer) in enumerate(questions, 1):
        print(f"Question {i}\n {question}\nAnswer: {answer}\n")



if __name__ == "__main__":
    main()